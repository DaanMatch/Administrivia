# Data Pipeline

```{admonition} Goal
Set of processes for:
- Extracting data from data sources
- Transforming data
- Loading into an output destination like data warehouse
From there data can be consumed for training or making predictions with ML models
```


Batch Data
: Batch processing can be done on data available in huge volumes in data lakes, from csv files, og files etc.

Streaming Data
: Real-time streaming data, like data from sensors.

1. Before data is used for making batch predictions:
It has to be extracted from multiple sources like log files, steraming sources, APIs, apps etc.
2. Transformed
3. Loaded into a data base for prediction.
This is done through ETL pipelines.



